# -*- coding: utf-8 -*-
"""creditcardfrauddetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10znzlJLR8_xRESHiPDfW9AkYt6ScSNdw
"""

!pip install imbalanced-learn xgboost lightgbm scikit-learn matplotlib seaborn

"""## Presentation Outline

Here is a suggested outline for presenting your work:

1.  **Introduction:** Briefly explain the problem (credit card fraud detection) and the goal of the project.
2.  **Data Loading and Exploration:**
    *   Mention the dataset used (`creditcard.csv`).
    *   Show the shape of the data and the first few rows (`df.head()`).
    *   Discuss the class distribution and the issue of imbalanced data (`df['Class'].value_counts()`, `sns.countplot`).
3.  **Data Preprocessing:**
    *   Explain the train-test split (`train_test_split`).
    *   Discuss the handling of imbalanced data using SMOTE and RandomUnderSampler (`SMOTE`, `RandomUnderSampler`, `y_train.value_counts()` before and after resampling).
    *   Explain the need for scaling and the use of `StandardScaler`.
4.  **Model Training and Evaluation:**
    *   List the models used (Logistic Regression, Random Forest, XGBoost, LightGBM).
    *   Explain the `evaluate_model` function and the metrics used (Classification Report, ROC-AUC, PR-AUC, Confusion Matrix).
    *   Show the results of the models trained on SMOTE data (`results_df`).
    *   If you evaluate models on undersampled data, present those results as well.
5.  **Model Comparison:** Compare the performance of the models based on the chosen metrics.
6.  **Future Work (Optional):** Discuss potential next steps, such as hyperparameter tuning, using different resampling techniques, or exploring other models.
7.  **Conclusion:** Summarize the key findings and insights.

## Libraries Used

Here are the main Python libraries used in this notebook:

*   **pandas:** For data manipulation and analysis.
*   **numpy:** For numerical operations.
*   **matplotlib.pyplot:** For creating static visualizations.
*   **seaborn:** For creating statistical graphics.
*   **sklearn (scikit-learn):** A comprehensive library for machine learning, including:
    *   `model_selection`: For splitting data (`train_test_split`).
    *   `preprocessing`: For scaling data (`StandardScaler`).
    *   `linear_model`: For Logistic Regression.
    *   `ensemble`: For Random Forest.
    *   `metrics`: For evaluating model performance (classification report, confusion matrix, ROC-AUC, PR-AUC, AUC).
*   **xgboost:** An optimized distributed gradient boosting library.
*   **lightgbm:** A gradient boosting framework that uses tree-based learning algorithms.
*   **imblearn (imbalanced-learn):** A library for dealing with imbalanced datasets, including:
    *   `over_sampling`: For oversampling techniques like SMOTE.
    *   `under_sampling`: For undersampling techniques like RandomUnderSampler.
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.metrics import (classification_report, confusion_matrix,
                             roc_auc_score, precision_recall_curve, auc)
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

import pandas as pd

df = pd.read_csv("creditcard.csv")   # no /data/ needed
print("Shape:", df.shape)
df.head()

df = pd.read_csv("creditcard.csv")
print("Shape:", df.shape)
print(df.head())

print("\nClass distribution:\n", df['Class'].value_counts())
plt.figure(figsize=(5,4))
sns.countplot(x="Class", data=df)
plt.title("Class distribution (0 = Normal, 1 = Fraud)")
plt.show()

X = df.drop("Class", axis=1)
y = df["Class"]

# Drop rows where 'Class' is NaN
df.dropna(subset=['Class'], inplace=True)
X = df.drop("Class", axis=1)
y = df["Class"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

smote = SMOTE(random_state=42)
X_train_res_smote, y_train_res_smote = smote.fit_resample(X_train, y_train)

# --- Undersampling (GitHub style) ---
rus = RandomUnderSampler(random_state=42)
X_train_res_rus, y_train_res_rus = rus.fit_resample(X_train, y_train)

print("\nOriginal Class Distribution:\n", y_train.value_counts())
print("After SMOTE:\n", y_train_res_smote.value_counts())
print("After Undersampling:\n", y_train_res_rus.value_counts())

scaler = StandardScaler()
X_train_res_smote = scaler.fit_transform(X_train_res_smote)
X_train_res_rus = scaler.fit_transform(X_train_res_rus)
X_test_scaled = scaler.transform(X_test)

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42),
    "LightGBM": LGBMClassifier(random_state=42)
}

def evaluate_model(model, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:,1]

    # Metrics
    roc = roc_auc_score(y_test, y_proba)
    precision, recall, _ = precision_recall_curve(y_test, y_proba)
    pr_auc = auc(recall, precision)

    print(classification_report(y_test, y_pred))
    print("ROC-AUC:", roc)
    print("PR-AUC:", pr_auc)

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap="Blues")
    plt.title("Confusion Matrix")
    plt.show()

    return roc, pr_auc

results = {}

for name, model in models.items():
    print("\n============================")
    print(f" Model: {name} (SMOTE)")
    print("============================")
    roc, pr = evaluate_model(model, X_train_res_smote, y_train_res_smote, X_test_scaled, y_test)
    results[name] = {"ROC-AUC": roc, "PR-AUC": pr}

results_df = pd.DataFrame(results).T
print("\nFinal Results Comparison:\n")
print(results_df)

results_df.plot(kind="bar", figsize=(8,5))
plt.title("Model Comparison (SMOTE)")
plt.ylabel("Score")
plt.show()

# Handle class imbalance using SMOTE (Oversampling) and RandomUnderSampler (Undersampling)
smote = SMOTE(random_state=42)
X_train_res_smote, y_train_res_smote = smote.fit_resample(X_train, y_train)

rus = RandomUnderSampler(random_state=42)
X_train_res_rus, y_train_res_rus = rus.fit_resample(X_train, y_train)

print("Original Class Distribution:\n", y_train.value_counts())
print("After SMOTE:\n", y_train_res_smote.value_counts())
print("After Undersampling:\n", y_train_res_rus.value_counts())

# Scale the features
scaler = StandardScaler()
X_train_res_smote = scaler.fit_transform(X_train_res_smote)
X_train_res_rus = scaler.fit_transform(X_train_res_rus)
X_test_scaled = scaler.transform(X_test)

# Define models to train
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42),
    "LightGBM": LGBMClassifier(random_state=42)
}

# Compare results
results_df = pd.DataFrame(results).T
print("\nFinal Results Comparison:\n")
print(results_df)

results_df.plot(kind="bar", figsize=(8,5))
plt.title("Model Comparison (SMOTE)")
plt.ylabel("Score")
plt.show()

# Handle class imbalance using SMOTE (Oversampling) and RandomUnderSampler (Undersampling)
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

smote = SMOTE(random_state=42)
X_train_res_smote, y_train_res_smote = smote.fit_resample(X_train, y_train)

rus = RandomUnderSampler(random_state=42)
X_train_res_rus, y_train_res_rus = rus.fit_resample(X_train, y_train)

print("Original Class Distribution:\n", y_train.value_counts())
print("After SMOTE:\n", y_train_res_smote.value_counts())
print("After Undersampling:\n", y_train_res_rus.value_counts())

# Scale the features
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_res_smote = scaler.fit_transform(X_train_res_smote)
X_train_res_rus = scaler.fit_transform(X_train_res_rus)
X_test_scaled = scaler.transform(X_test)

X = df.drop("Class", axis=1)
y = df["Class"]

# Drop rows where 'Class' is NaN
df.dropna(subset=['Class'], inplace=True)
X = df.drop("Class", axis=1)
y = df["Class"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

import pandas as pd

df = pd.read_csv("creditcard.csv")   # no /data/ needed
print("Shape:", df.shape)
df.head()

from sklearn.model_selection import train_test_split

X = df.drop("Class", axis=1)
y = df["Class"]

# Drop rows where 'Class' is NaN
df.dropna(subset=['Class'], inplace=True)
X = df.drop("Class", axis=1)
y = df["Class"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

# Handle class imbalance using SMOTE (Oversampling) and RandomUnderSampler (Undersampling)
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler

smote = SMOTE(random_state=42)
X_train_res_smote, y_train_res_smote = smote.fit_resample(X_train, y_train)

rus = RandomUnderSampler(random_state=42)
X_train_res_rus, y_train_res_rus = rus.fit_resample(X_train, y_train)

print("Original Class Distribution:\n", y_train.value_counts())
print("After SMOTE:\n", y_train_res_smote.value_counts())
print("After Undersampling:\n", y_train_res_rus.value_counts())

# Scale the features
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_res_smote = scaler.fit_transform(X_train_res_smote)
X_train_res_rus = scaler.fit_transform(X_train_res_rus)
X_test_scaled = scaler.transform(X_test)



"""Based on the requirements and the completed steps, here are the remaining tasks:

1.  **Evaluate models with Undersampling:** Evaluate the performance of the trained models using the undersampled data.
2.  **Compare Results:** Compare the performance of the models using both SMOTE and undersampling.
3.  **Optimize Model:** Optimize the best-performing model using hyperparameter tuning.
4.  **Visualize Patterns:** Visualize fraud vs non-fraud patterns using PCA/TSNE.
5.  **Summarize Findings:** Summarize the findings and conclude the analysis.
"""

results_rus = {}

for name, model in models.items():
    print("\n============================")
    print(f" Model: {name} (Undersampling)")
    print("============================")
    roc, pr = evaluate_model(model, X_train_res_rus, y_train_res_rus, X_test_scaled, y_test)
    results_rus[name] = {"ROC-AUC": roc, "PR-AUC": pr}

# Define models to train
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier

models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, random_state=42),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric="logloss", random_state=42),
    "LightGBM": LGBMClassifier(random_state=42)
}
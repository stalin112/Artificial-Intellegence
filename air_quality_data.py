# -*- coding: utf-8 -*-
"""Air quality data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jKNK4tWMZYcYFFEqJxUiYzkR4w_fr6e_
"""

import os

# List all files in the current directory
files = os.listdir('.')

# Print the list of files
print("Files in the current directory:")
for file in files:
    print(file)

import pandas as pd
import os

file_path = 'stations.csv'
if not os.path.exists(file_path):
    print(f"File not found: {file_path}. Please upload the file to the Colab environment.")
else:
    df_stations = pd.read_csv(file_path)
    df_station_day = pd.read_csv('station_day.csv')
    df_city_hour = pd.read_csv('city_hour.csv')
    df_station_hour = pd.read_csv('station_hour.csv')
    df_city_day = pd.read_csv('city_day.csv')

import pandas as pd
import os

file_path = 'stations.csv'
if not os.path.exists(file_path):
    print(f"File not found: {file_path}. Please upload the file to the Colab environment.")
else:
    df_stations = pd.read_csv(file_path)
    df_station_day = pd.read_csv('station_day.csv')
    df_city_hour = pd.read_csv('city_hour.csv')
    df_station_hour = pd.read_csv('station_hour.csv')
    df_city_day = pd.read_csv('city_day.csv')

dataframes = {
    'df_stations': df_stations,
    'df_station_day': df_station_day,
    'df_city_hour': df_city_hour,
    'df_station_hour': df_station_hour,
    'df_city_day': df_city_day
}

for name, df in dataframes.items():
    print(f"--- Cleaning and exploring {name} ---")

    # Check for missing values
    print("Missing values per column:")
    print(df.isnull().sum())

    # Check for duplicate rows
    print("\nNumber of duplicate rows:")
    print(df.duplicated().sum())

    # Inspect data types
    print("\nData types:")
    df.info()

    # Convert 'Datetime' column to datetime objects if it exists
    if 'Datetime' in df.columns:
        df[
            'Datetime'
        ] = pd.to_datetime(df['Datetime'])
        print("\n'Datetime' column converted to datetime objects.")

    print("\n" + "="*40 + "\n")

for name, df in dataframes.items():
    if name == 'df_stations':
        continue

    print(f"--- Exploring {name} ---")

    print("\nFirst 5 rows:")
    display(df.head())

    print("\nDescriptive statistics for numerical columns:")
    display(df.describe())

    print("\nValue counts for categorical columns:")
    for col in df.select_dtypes(include='object').columns:
        print(f"\nValue counts for '{col}':")
        display(df[col].value_counts())

    if 'Datetime' in df.columns:
        print("\nDate Range:")
        print(f"Minimum Date: {df['Datetime'].min()}")
        print(f"Maximum Date: {df['Datetime'].max()}")

    print("\n" + "="*40 + "\n")

"""## Presentation Outline: Air Quality Data Analysis Project

Here is a suggested outline for presenting your air quality data analysis project to your internship reviewer:

1.  **Introduction (5-10 minutes)**
    *   Briefly introduce the project and its objective: Analyzing air quality data to identify trends, patterns, and relationships between pollutants and AQI.
    *   Explain the importance of air quality monitoring and analysis (e.g., public health, environmental impact).
    *   Mention the data sources used ("stations.csv", "station_day.csv", "city_hour.csv", "station_hour.csv", "city_day.csv").

2.  **Data Loading and Cleaning (10-15 minutes)**
    *   Explain how you loaded the data into pandas DataFrames.
    *   Discuss the data cleaning steps you performed:
        *   Checking for missing values.
        *   Handling duplicates.
        *   Correcting data types (specifically converting 'Datetime' columns).
    *   Highlight any issues found during cleaning (e.g., missing values in `df_city_hour`) and how you addressed them (or plan to address them).

3.  **Data Exploration (15-20 minutes)**
    *   Present key findings from your data exploration:
        *   Summary statistics of numerical columns.
        *   Unique values and distributions of categorical columns.
        *   Date ranges of the datasets.
    *   Share any interesting initial observations about the data.

4.  **Data Visualization (15-20 minutes)**
    *   Show the visualizations you created (histograms, scatter plots, time series plots).
    *   Explain what each visualization reveals about the data (e.g., distribution of pollutants, relationship between PM2.5 and PM10, AQI trends over time).
    *   Discuss any patterns or anomalies observed in the plots.

5.  **Specific Analysis / Time Series Analysis / Statistical Modeling (15-20 minutes)**
    *   Explain your time series analysis approach and what you learned about trends and seasonality in AQI or pollutant levels for different cities.
    *   Discuss your attempt at statistical modeling to predict AQI. Explain the model you chose (linear regression) and the results (including the R-squared value and what it signifies). Discuss potential reasons for the model's performance and future steps (e.g., trying other models).
    *   Present the comparison of air quality metrics across different cities and stations and discuss any notable differences.

6.  **Summary and Next Steps (5-10 minutes)**
    *   Summarize the key findings of your analysis.
    *   Discuss potential next steps for the project (e.g., handling missing values, exploring other modeling techniques, investigating seasonality further).
    *   Open the floor for questions.

## Code Explanation

Here's an explanation of the main modules and code snippets you've used, which you can use to guide your reviewer through the notebook:

### 1. Data Loading (`pandas`)

**Module:** `pandas`

**Explanation:** Pandas is a powerful library for data manipulation and analysis. It provides data structures like DataFrames, which are like tables, making it easy to work with structured data.

**Code:**

### 2. Data Cleaning and Exploration (`pandas`)

**Module:** `pandas`

**Explanation:** You continue to use pandas for data cleaning and initial exploration.

**Code:**
"""

for name, df in dataframes.items():
    if name == 'df_stations':
        continue
    print(f"--- Exploring {name} ---")
    print("\nFirst 5 rows:")
    display(df.head())
    print("\nDescriptive statistics for numerical columns:")
    display(df.describe())
    print("\nValue counts for categorical columns:")
    for col in df.select_dtypes(include='object').columns:
        print(f"\nValue counts for '{col}':")
        display(df[col].value_counts())
    if 'Datetime' in df.columns:
        print("\nDate Range:")
        print(f"Minimum Date: {df['Datetime'].min()}")
        print(f"Maximum Date: {df['Datetime'].max()}")
    print("\n" + "="*40 + "\n")

"""### 3. Data Visualization (`matplotlib.pyplot`, `seaborn`)

**Modules:** `matplotlib.pyplot`, `seaborn`

**Explanation:** These libraries are used for creating static, interactive, and animated visualizations in Python. `Matplotlib` is the foundational library, while `Seaborn` is built on top of it and provides a higher-level interface for drawing attractive statistical graphics.

**Code:**

### 4. Statistical Modeling (`sklearn`)

**Module:** `sklearn` (scikit-learn)

**Explanation:** Scikit-learn is a widely used library for machine learning in Python. It provides tools for various tasks, including regression, classification, clustering, and model evaluation.

**Code:**

### 5. Comparing Cities/Stations (`pandas`, `seaborn`)

**Modules:** `pandas`, `seaborn`

**Explanation:** You use pandas for data aggregation and seaborn for visualization to compare air quality across different locations.

**Code:**

This explanation covers the main parts of your code and the modules you've used. Remember to practice explaining each section in your own words and be prepared to answer questions about your findings and the choices you made during the analysis. Good luck with your internship review!

# Task
Load the data from "stations.csv", "station_day.csv", "city_hour.csv", "station_hour.csv", and "city_day.csv" into pandas DataFrames, clean and explore the data, and provide code examples for data visualization and analysis.

## Load the data

### Subtask:
Load the data from "stations.csv", "station_day.csv", "city_hour.csv", "station_hour.csv", and "city_day.csv" into pandas DataFrames.

**Reasoning**:
The subtask is to load multiple CSV files into pandas DataFrames. This requires importing the pandas library and then using the `read_csv` function for each file.
"""

import pandas as pd

df_stations = pd.read_csv('stations.csv')
df_station_day = pd.read_csv('station_day.csv')
df_city_hour = pd.read_csv('city_hour.csv')
df_station_hour = pd.read_csv('station_hour.csv')
df_city_day = pd.read_csv('city_day.csv')

"""## Data cleaning

### Subtask:
Clean the loaded DataFrames by checking for missing values, handling duplicates, and correcting data types.

**Reasoning**:
Iterate through each DataFrame to check for missing values and duplicates, inspect data types, and convert 'Datetime' columns to datetime objects as instructed.
"""

dataframes = {
    'df_stations': df_stations,
    'df_station_day': df_station_day,
    'df_city_hour': df_city_hour,
    'df_station_hour': df_station_hour,
    'df_city_day': df_city_day
}

for name, df in dataframes.items():
    print(f"--- Cleaning and exploring {name} ---")

    # Check for missing values
    print("Missing values per column:")
    print(df.isnull().sum())

    # Check for duplicate rows
    print("\nNumber of duplicate rows:")
    print(df.duplicated().sum())

    # Inspect data types
    print("\nData types:")
    df.info()

    # Convert 'Datetime' column to datetime objects if it exists
    if 'Datetime' in df.columns:
        df[
            'Datetime'
        ] = pd.to_datetime(df['Datetime'])
        print("\n'Datetime' column converted to datetime objects.")

    print("\n" + "="*40 + "\n")

"""## Data exploration

### Subtask:
Explore the data by looking at summary statistics, unique values, and distributions of key variables.

**Reasoning**:
Iterate through the dataframes (excluding `df_stations`), display the head, describe numerical columns, and check value counts for categorical columns. Also, explore the date range for dataframes with a 'Datetime' column.
"""

for name, df in dataframes.items():
    if name == 'df_stations':
        continue

    print(f"--- Exploring {name} ---")

    print("\nFirst 5 rows:")
    display(df.head())

    print("\nDescriptive statistics for numerical columns:")
    display(df.describe())

    print("\nValue counts for categorical columns:")
    for col in df.select_dtypes(include='object').columns:
        print(f"\nValue counts for '{col}':")
        display(df[col].value_counts())

    if 'Datetime' in df.columns:
        print("\nDate Range:")
        print(f"Minimum Date: {df['Datetime'].min()}")
        print(f"Maximum Date: {df['Datetime'].max()}")

    print("\n" + "="*40 + "\n")

"""## Data visualization

### Subtask:
Create visualizations (e.g., histograms, scatter plots, time series plots) to understand the relationships between variables and identify patterns.

**Reasoning**:
Create visualizations as requested in the instructions using the loaded dataframes.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Choose df_city_day for visualization
df_viz = df_city_day.copy()

# 3. Create a histogram for a numerical column (e.g., 'PM2.5')
plt.figure(figsize=(10, 6))
sns.histplot(df_viz['PM2.5'].dropna(), bins=50, kde=True)
plt.title('Distribution of PM2.5')
plt.xlabel('PM2.5')
plt.ylabel('Frequency')
plt.show()

# 4. Create a scatter plot for 'PM2.5' and 'PM10', colored by city
plt.figure(figsize=(12, 8))
sns.scatterplot(data=df_viz, x='PM2.5', y='PM10', hue='City', alpha=0.6)
plt.title('PM2.5 vs PM10 by City')
plt.xlabel('PM2.5')
plt.ylabel('PM10')
plt.show()

# 5. Create a time series plot for AQI over time for a specific city (e.g., Delhi)
df_delhi = df_viz[df_viz['City'] == 'Delhi'].sort_values('Datetime')
plt.figure(figsize=(15, 7))
sns.lineplot(data=df_delhi, x='Datetime', y='AQI')
plt.title('AQI over Time in Delhi')
plt.xlabel('Datetime')
plt.ylabel('AQI')
plt.show()

"""## Specific analysis (optional)

### Subtask:
Perform a correlation analysis on the numerical columns of the `df_city_day` DataFrame to understand the relationships between different air pollutants and AQI.

**Reasoning**:
Select numerical columns, calculate the correlation matrix, and visualize it using a heatmap.

## Document findings

### Subtask:
Summarize the key findings from the data cleaning, exploration, visualization, and specific analysis steps.

## Summary:

### Data Analysis Key Findings

*   **Missing Values:** `df_city_hour` has missing values in 'SO2', 'O3', 'Benzene', 'Toluene', 'Xylene', 'AQI', and 'AQI\_Bucket' columns. Other DataFrames (`df_stations`, `df_station_day`, `df_station_hour`, `df_city_day`) have no missing values.
*   **Duplicate Data:** No duplicate rows were found in any of the DataFrames.
*   **Data Types:** 'Datetime' columns in `df_station_day`, `df_city_hour`, `df_station_hour`, and `df_city_day` were successfully converted to datetime objects.
*   **Temporal Coverage:**
    *   `df_station_day` and `df_city_day` cover a longer period from 2015-01-01 to 2024-12-31.
    *   `df_city_hour` covers from 2015-01-01 00:00:00 to 2020-08-31 19:00:00.
    *   `df_station_hour` has a shorter period shown in the exploration output, from 2015-01-01 00:00:00 to 2015-01-31 00:00:00.
*   **PM2.5 Distribution:** The distribution of PM2.5 shows a range of frequencies across different concentration levels.
*   **PM2.5 vs PM10:** There appears to be a positive correlation between PM2.5 and PM10 concentrations, with potential variations in this relationship across different cities.
*   **AQI Trend in Delhi:** The time series plot for Delhi shows fluctuations in AQI over time, indicating periods of higher and lower air quality.
*   **Correlations:** A correlation analysis on the numerical columns of `df_city_day` reveals the strength and direction of relationships between various air pollutants and AQI. (Specific correlation values are shown in the generated heatmap).

### Insights or Next Steps

*   Investigate the cause and impact of missing values in `df_city_hour` and apply an appropriate imputation strategy.
*   Further explore the city-specific patterns observed in the PM2.5 vs PM10 scatter plot to understand regional differences in pollutant relationships.

# Task
Analyze the air quality data from the provided CSV files ("stations.csv", "station_day.csv", "city_hour.csv", "station_hour.csv", "city_day.csv") to identify trends, patterns, and relationships between pollutants and AQI. Provide code for data loading, cleaning, exploration, visualization, and time series analysis to help with an internship project.

## Time series analysis

### Subtask:
Analyze trends and seasonality in AQI or specific pollutant levels over time for different cities or stations.

**Reasoning**:
Iterate through each city in `df_city_day` and create a time series plot for AQI.
"""

for city in df_city_day['City'].unique():
    df_city = df_city_day[df_city_day['City'] == city].sort_values('Datetime')
    plt.figure(figsize=(15, 7))
    sns.lineplot(data=df_city, x='Datetime', y='AQI')
    plt.title(f'AQI over Time in {city}')
    plt.xlabel('Datetime')
    plt.ylabel('AQI')
    plt.show()

"""**Reasoning**:
Repeat the time series plotting process for PM2.5 for each city as requested in the instructions.


"""

for city in df_city_day['City'].unique():
    df_city = df_city_day[df_city_day['City'] == city].sort_values('Datetime')
    plt.figure(figsize=(15, 7))
    sns.lineplot(data=df_city, x='Datetime', y='PM2.5')
    plt.title(f'PM2.5 over Time in {city}')
    plt.xlabel('Datetime')
    plt.ylabel('PM2.5')
    plt.show()

"""## Geospatial analysis (if applicable)

### Subtask:
If station locations are available, analyze spatial patterns of air quality.

## Statistical modeling (optional)

### Subtask:
Build models to predict AQI based on pollutant levels or other factors.

**Reasoning**:
Select relevant columns for modeling, handle missing values, split the data, train a regression model, and evaluate its performance.

## Compare cities/stations

### Subtask:
Compare air quality metrics and trends across different cities or stations.

**Reasoning**:
Calculate and display the average AQI and average levels of key pollutants for each city and station, then visualize these averages using bar plots.

## Investigate pollutant relationships

### Subtask:
Analyze the relationships between different pollutants and their impact on AQI in more detail.

**Reasoning**:
Select numerical columns, calculate the correlation matrix, and visualize it using a heatmap to analyze the relationships between different pollutants and AQI.
"""